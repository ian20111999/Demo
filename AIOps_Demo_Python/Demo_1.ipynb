{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 20:21:46,960 - INFO - Starting system monitoring...\n",
      "2024-09-03 20:27:09,964 - INFO - Monitoring interrupted by user.\n",
      "2024-09-03 20:27:09,968 - INFO - Exiting monitoring loop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import logging\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import subprocess\n",
    "import time\n",
    "import platform\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from collections import deque\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "\n",
    "# 应用 nest_asyncio 以允许嵌套事件循环\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 设置日志并启用日志轮转\n",
    "log_handler = RotatingFileHandler('system_monitor.log', maxBytes=5000000, backupCount=5)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[log_handler])\n",
    "\n",
    "# 数据队列大小设置\n",
    "MAX_METRICS_LEN = 600  # 10分钟的数据\n",
    "\n",
    "# 警示阈值\n",
    "ALERT_THRESHOLD = 20\n",
    "\n",
    "# 收集系统性能数据\n",
    "def collect_system_metrics(num_samples=10, interval=1):\n",
    "    metrics_list = []\n",
    "    for _ in range(num_samples):\n",
    "        cpu_usage = psutil.cpu_percent(interval=interval)\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        memory_usage = memory_info.percent\n",
    "        disk_io = psutil.disk_io_counters()\n",
    "        network_io = psutil.net_io_counters()\n",
    "        metrics_list.append({\n",
    "            'timestamp': pd.Timestamp.now(),\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_usage': memory_usage,\n",
    "            'disk_read': disk_io.read_bytes,\n",
    "            'disk_write': disk_io.write_bytes,\n",
    "            'network_sent': network_io.bytes_sent,\n",
    "            'network_recv': network_io.bytes_recv\n",
    "        })\n",
    "    return metrics_list\n",
    "\n",
    "# 异常检测\n",
    "def detect_anomalies(data):\n",
    "    if len(data) < 50:  # 确保样本数足够\n",
    "        logging.info(\"Not enough data to perform anomaly detection.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data.drop(columns=['timestamp']))\n",
    "    \n",
    "    model = IsolationForest(contamination=0.01)  # 改进异常检测模型\n",
    "    scores = model.fit_predict(data_scaled)\n",
    "    anomaly_scores = (scores < 0).astype(int)\n",
    "    \n",
    "    anomalies = data[anomaly_scores > 0]\n",
    "    return anomalies\n",
    "\n",
    "# 防止重复修复机制\n",
    "last_remediation_time = 0\n",
    "remediation_cooldown = 5  # 冷却时间\n",
    "\n",
    "def execute_remediation_script(issue_type):\n",
    "    global last_remediation_time\n",
    "    current_time = time.time()\n",
    "    if current_time - last_remediation_time < remediation_cooldown:\n",
    "        logging.info(f\"Skipping remediation due to cooldown: {remediation_cooldown} seconds.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Executing remediation script for issue: {issue_type}...\")\n",
    "        if issue_type == \"high_cpu\":\n",
    "            subprocess.run([\"echo\", \"Simulating restart of critical_service\"], check=True)\n",
    "        elif issue_type == \"memory_leak\":\n",
    "            subprocess.run([\"echo\", \"Simulating killing of memory_hog_process\"], check=True)\n",
    "        if platform.system() == 'Darwin':\n",
    "            subprocess.run([\"osascript\", \"-e\", f'display notification \\\"Issue {issue_type} detected and remediated\\\" with title \\\"AIOps Alert\\\"'])\n",
    "        logging.info(\"Remediation script executed successfully.\")\n",
    "        last_remediation_time = current_time  # 更新修复时间\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Error occurred while executing remediation script: {e}\")\n",
    "\n",
    "# 创建BiLSTM模型以用于调参\n",
    "def build_bilstm_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(50, 6)))\n",
    "\n",
    "    # 调节LSTM单元数\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Bidirectional(LSTM(units=hp.Int(f'units_{i}', min_value=50, max_value=400, step=50),\n",
    "                                     activation='relu', return_sequences=True if i < 2 else False,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001))))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                  loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# 调参和训练\n",
    "def tune_and_train_model(X_train, y_train):\n",
    "    tuner = kt.Hyperband(build_bilstm_model,\n",
    "                         objective='val_loss',\n",
    "                         max_epochs=50,\n",
    "                         factor=3,\n",
    "                         directory='tuner_logs',\n",
    "                         project_name='bilstm_tuning')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# 数据预处理，构建BiLSTM模型，并进行预测\n",
    "def prepare_bilstm_model_and_predict(train_data, test_data, n_steps=50):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # 训练数据处理\n",
    "    train_data_scaled = train_data.copy()\n",
    "    features_to_scale = ['cpu_usage', 'memory_usage', 'disk_read', 'disk_write', 'network_sent', 'network_recv']\n",
    "    train_data_scaled[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "    \n",
    "    X_train, y_train = [], []\n",
    "    for i in range(n_steps, len(train_data_scaled)):\n",
    "        X_train.append(train_data_scaled.iloc[i-n_steps:i].drop(columns=['timestamp']).values)\n",
    "        y_train.append(train_data_scaled['cpu_usage'].iloc[i])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # 使用自动调参后的模型\n",
    "    model = tune_and_train_model(X_train, y_train)\n",
    "\n",
    "    # 测试数据处理\n",
    "    test_data_scaled = test_data.copy()\n",
    "    test_data_scaled[features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "    \n",
    "    X_test = []\n",
    "    for i in range(n_steps, len(test_data_scaled)):\n",
    "        X_test.append(test_data_scaled.iloc[i-n_steps:i].drop(columns=['timestamp']).values)\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "    # 预测\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # 检查 predictions 是否是二维的\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = predictions[:, :, 0]  # 如果是三维的，选择第三维度中的第一个元素\n",
    "\n",
    "    # `predictions` 现在应该是二维数组，形状为 (样本数, 1)\n",
    "    # 重新调整 `padding_shape` 的定义\n",
    "    padding_shape = (predictions.shape[0], len(features_to_scale) - 1)\n",
    "\n",
    "    # 确保填充的 zeros 数组和 predictions 有相同的维度\n",
    "    predictions_with_padding = np.concatenate([predictions, np.zeros(padding_shape)], axis=1)\n",
    "\n",
    "    # 对连接后的数组进行逆缩放\n",
    "    predictions = scaler.inverse_transform(predictions_with_padding)[:, 0]\n",
    "\n",
    "    # 检查预测值是否超过警示阈值\n",
    "    high_usage = predictions > ALERT_THRESHOLD\n",
    "    if np.any(high_usage):\n",
    "        logging.warning(\"High CPU usage predicted!\")\n",
    "        execute_remediation_script(\"potential_high_cpu\")\n",
    "\n",
    "    # 将预测值与时间戳进行合并\n",
    "    forecast = pd.DataFrame({\n",
    "        'timestamp': test_data['timestamp'].iloc[n_steps:].values,\n",
    "        'predicted_cpu_usage': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    return forecast\n",
    "\n",
    "\n",
    "# 实时监控系统性能\n",
    "async def monitor_system():\n",
    "    try:\n",
    "        logging.info(\"Starting system monitoring...\")\n",
    "        start_time = time.time()\n",
    "        max_duration = 600  # 10 分钟\n",
    "\n",
    "        metrics_list = deque(maxlen=MAX_METRICS_LEN)  # 使用双端队列限制数据大小\n",
    "\n",
    "        # 初始化空的DataFrame用于记录结果\n",
    "        results_df = pd.DataFrame()\n",
    "\n",
    "        while time.time() - start_time < max_duration:\n",
    "            metrics_batch = collect_system_metrics(num_samples=10)\n",
    "            metrics_list.extend(metrics_batch)\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "        logging.info(\"Data collection finished. Beginning analysis...\")\n",
    "\n",
    "        data = pd.DataFrame(metrics_list)\n",
    "\n",
    "        # 使用前80%的数据进行训练，后20%进行预测\n",
    "        train_size = int(len(data) * 0.8)\n",
    "        train_data = data.iloc[:train_size]\n",
    "        test_data = data.iloc[train_size:]\n",
    "\n",
    "        # 模型训练与预测\n",
    "        forecast = prepare_bilstm_model_and_predict(train_data, test_data)\n",
    "\n",
    "        # 合并真实数据和预测数据\n",
    "        merged_data = pd.merge_asof(test_data[['timestamp', 'cpu_usage']], forecast, on='timestamp')\n",
    "\n",
    "        # 绘制图表\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(merged_data['timestamp'], merged_data['cpu_usage'], label='实际', color='blue')\n",
    "        plt.plot(merged_data['timestamp'], merged_data['predicted_cpu_usage'], label='预测', color='red', linestyle='--')\n",
    "        plt.axhline(y=ALERT_THRESHOLD, color='green', linestyle=':', label='警示阈值')\n",
    "        plt.xlabel('时间')\n",
    "        plt.ylabel('CPU 使用率')\n",
    "        plt.title('CPU 使用率 - 实际 vs 预测（最后2分钟）')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Monitoring interrupted by user.\")\n",
    "    finally:\n",
    "        logging.info(\"Exiting monitoring loop.\")\n",
    "\n",
    "# 启动监控系统\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(monitor_system())\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Program terminated by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 56\u001b[0m\n\u001b[1;32m     46\u001b[0m         metrics_batch\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mTimestamp\u001b[38;5;241m.\u001b[39mnow(),\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu_usage\u001b[39m\u001b[38;5;124m'\u001b[39m: cpu_usage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork_recv\u001b[39m\u001b[38;5;124m'\u001b[39m: network_io\u001b[38;5;241m.\u001b[39mbytes_recv\n\u001b[1;32m     54\u001b[0m         })\n\u001b[1;32m     55\u001b[0m     metrics_list\u001b[38;5;241m.\u001b[39mextend(metrics_batch)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData collection finished. Beginning analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 将收集到的数据转换为DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/asyncio/tasks.py:609\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    605\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    606\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    607\u001b[0m                     future, result)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/asyncio/futures.py:284\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/asyncio/futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[1;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import logging\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import subprocess\n",
    "import time\n",
    "import platform\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from collections import deque\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 应用 nest_asyncio 以允许嵌套事件循环\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 设置日志并启用日志轮转\n",
    "log_handler = RotatingFileHandler('system_monitor.log', maxBytes=5000000, backupCount=5)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[log_handler])\n",
    "\n",
    "# 数据队列大小设置\n",
    "MAX_METRICS_LEN = 600  # 10分钟的数据\n",
    "ALERT_THRESHOLD = 20  # 警示阈值\n",
    "\n",
    "# 初始化数据队列\n",
    "metrics_list = deque(maxlen=MAX_METRICS_LEN)  # 使用双端队列限制数据大小\n",
    "start_time = time.time()\n",
    "max_duration = 600  # 10 分钟\n",
    "\n",
    "# 监控和收集数据\n",
    "while time.time() - start_time < max_duration:\n",
    "    metrics_batch = []\n",
    "    for _ in range(10):  # 每次收集10个样本\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        memory_usage = memory_info.percent\n",
    "        disk_io = psutil.disk_io_counters()\n",
    "        network_io = psutil.net_io_counters()\n",
    "        metrics_batch.append({\n",
    "            'timestamp': pd.Timestamp.now(),\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_usage': memory_usage,\n",
    "            'disk_read': disk_io.read_bytes,\n",
    "            'disk_write': disk_io.write_bytes,\n",
    "            'network_sent': network_io.bytes_sent,\n",
    "            'network_recv': network_io.bytes_recv\n",
    "        })\n",
    "    metrics_list.extend(metrics_batch)\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "logging.info(\"Data collection finished. Beginning analysis...\")\n",
    "\n",
    "# 将收集到的数据转换为DataFrame\n",
    "data = pd.DataFrame(metrics_list)\n",
    "\n",
    "# 使用前80%的数据进行训练，后20%进行预测\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data = data.iloc[:train_size]\n",
    "test_data = data.iloc[train_size:]\n",
    "\n",
    "# 数据预处理与模型训练\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# 训练数据处理\n",
    "train_data['cpu_usage'] = scaler.fit_transform(train_data[['cpu_usage']])\n",
    "X_train, y_train = [], []\n",
    "n_steps = 50\n",
    "for i in range(n_steps, len(train_data)):\n",
    "    X_train.append(train_data['cpu_usage'].iloc[i-n_steps:i].values)\n",
    "    y_train.append(train_data['cpu_usage'].iloc[i])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# 构建BiLSTM模型\n",
    "model = Sequential([\n",
    "    Input(shape=(n_steps, 1)),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Bidirectional(LSTM(400, activation='relu', return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(200, activation='relu', return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(100, activation='relu', return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(50, activation='relu')),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001), loss='mse')\n",
    "\n",
    "# 训练模型\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# 测试数据处理\n",
    "test_data['cpu_usage'] = scaler.transform(test_data[['cpu_usage']])\n",
    "X_test = []\n",
    "for i in range(n_steps, len(test_data)):\n",
    "    X_test.append(test_data['cpu_usage'].iloc[i-n_steps:i].values)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# 预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 确保 predictions 是二维的\n",
    "if predictions.ndim == 3:\n",
    "    predictions = predictions[:, :, 0]  # 如果是三维的，选择第三维度中的第一个元素\n",
    "\n",
    "# 将预测值与其他特征合并后进行逆缩放\n",
    "padding_shape = (predictions.shape[0], 1)\n",
    "predictions_with_padding = np.concatenate([predictions, np.zeros(padding_shape)], axis=1)\n",
    "predictions = scaler.inverse_transform(predictions_with_padding)[:, 0]\n",
    "\n",
    "# 检查预测值是否超过警示阈值\n",
    "high_usage = predictions > ALERT_THRESHOLD\n",
    "if np.any(high_usage):\n",
    "    logging.warning(\"High CPU usage predicted!\")\n",
    "    subprocess.run([\"echo\", \"Simulating restart of critical_service\"], check=True)\n",
    "\n",
    "# 合并预测值和实际值\n",
    "forecast = pd.DataFrame({'timestamp': test_data['timestamp'].iloc[n_steps:].values,\n",
    "                         'predicted_cpu_usage': predictions.flatten()})\n",
    "\n",
    "# 绘制图表\n",
    "merged_data = pd.merge_asof(test_data[['timestamp', 'cpu_usage']], forecast, on='timestamp')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_data['timestamp'], merged_data['cpu_usage'], label='实际', color='blue')\n",
    "plt.plot(merged_data['timestamp'], merged_data['predicted_cpu_usage'], label='预测', color='red', linestyle='--')\n",
    "plt.axhline(y=ALERT_THRESHOLD, color='green', linestyle=':', label='警示阈值')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('CPU 使用率')\n",
    "plt.title('CPU 使用率 - 实际 vs 预测（最后2分钟）')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 13m 43s]\n",
      "val_loss: nan\n",
      "\n",
      "Best val_loss So Far: nan\n",
      "Total elapsed time: 06h 34m 18s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "200               |200               |n_steps\n",
      "128               |32                |conv_filters\n",
      "4                 |2                 |lstm_layers\n",
      "400               |300               |lstm_units_0\n",
      "0.2               |0.1               |dropout_rate_0\n",
      "0.0001            |0.01              |learning_rate\n",
      "300               |100               |lstm_units_1\n",
      "0.1               |0.1               |dropout_rate_1\n",
      "400               |None              |lstm_units_2\n",
      "0.1               |None              |dropout_rate_2\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - loss: nan - val_loss: nan - learning_rate: 2.5000e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# 模型搜索与训练\u001b[39;00m\n\u001b[1;32m     93\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 获取最佳模型\u001b[39;00m\n\u001b[1;32m     97\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:339\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:588\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:545\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    549\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# 生成模拟的时间序列数据\n",
    "np.random.seed(42)\n",
    "timestamps = pd.date_range(start=\"2024-01-01\", periods=10000, freq=\"S\")\n",
    "cpu_usage = np.sin(np.linspace(0, 50, 10000)) + np.random.normal(0, 0.1, 10000) + 0.5\n",
    "\n",
    "data = pd.DataFrame({\"timestamp\": timestamps, \"cpu_usage\": cpu_usage})\n",
    "\n",
    "# 分割数据为训练集和测试集\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data = data.iloc[:train_size].copy()\n",
    "test_data = data.iloc[train_size:].copy()\n",
    "\n",
    "# 删除时间戳列，确保只使用数值数据\n",
    "train_data = train_data[['cpu_usage']]\n",
    "\n",
    "# 数据预处理\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data['cpu_usage'] = scaler.fit_transform(train_data[['cpu_usage']])\n",
    "\n",
    "# 定义模型构建函数\n",
    "def build_model(hp):\n",
    "    # 动态调整n_steps\n",
    "    n_steps = hp.Int('n_steps', min_value=200, max_value=400, step=100)\n",
    "    \n",
    "    # 根据n_steps生成训练数据\n",
    "    X_train, y_train = [], []\n",
    "    for i in range(n_steps, len(train_data)):\n",
    "        X_train.append(train_data['cpu_usage'].iloc[i-n_steps:i].values)\n",
    "        y_train.append(train_data['cpu_usage'].iloc[i])\n",
    "\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # 构建模型\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps, 1)))\n",
    "    model.add(Conv1D(filters=hp.Int('conv_filters', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    for i in range(hp.Int('lstm_layers', 1, 6)):\n",
    "        model.add(Bidirectional(LSTM(units=hp.Int(f'lstm_units_{i}', min_value=100, max_value=400, step=100),\n",
    "                                     activation='relu', return_sequences=(i != hp.Int('lstm_layers', 1, 6) - 1),\n",
    "                                     kernel_regularizer=l2(0.001))))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # 增加学习率调度策略\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 使用Keras Tuner进行超参数搜索\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=15,  # 增加搜索的trial数量\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='cpu_usage_prediction_v2'\n",
    ")\n",
    "\n",
    "# 使用EarlyStopping回调\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# 生成训练数据\n",
    "X_train, y_train = [], []\n",
    "n_steps = 200  # 设置初始的n_steps\n",
    "for i in range(n_steps, len(train_data)):\n",
    "    X_train.append(train_data['cpu_usage'].iloc[i-n_steps:i].values)\n",
    "    y_train.append(train_data['cpu_usage'].iloc[i])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# 模型搜索与训练\n",
    "tuner.search_space_summary()\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "n_steps = best_hyperparameters.get('n_steps')\n",
    "\n",
    "# 准备测试数据\n",
    "test_data['cpu_usage'] = scaler.transform(test_data[['cpu_usage']])\n",
    "X_test, y_test = [], []\n",
    "for i in range(n_steps, len(test_data)):\n",
    "    X_test.append(test_data['cpu_usage'].iloc[i-n_steps:i].values)\n",
    "    y_test.append(test_data['cpu_usage'].iloc[i])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# 预测\n",
    "predictions = best_model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# 确保 `predictions` 和 `y_test` 的形状一致\n",
    "predictions = predictions.flatten()\n",
    "y_test = y_test[:len(predictions)]  # 确保测试数据长度匹配\n",
    "\n",
    "# 确保时间戳和预测值对齐\n",
    "test_timestamps = test_data['timestamp'].iloc[n_steps:n_steps+len(predictions)].values\n",
    "\n",
    "# 绘制实际值与预测值的图表\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_timestamps, y_test, label='实际值', color='blue')\n",
    "plt.plot(test_timestamps, predictions, label='预测值', color='red', linestyle='--')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('CPU 使用率')\n",
    "plt.title('CPU 使用率 - 实际值 vs 预测值')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
