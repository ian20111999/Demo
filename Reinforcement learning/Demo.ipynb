{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from gym) (1.26.4)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827623 sha256=62a2f2b3427dea713638c468fa4a3adc2a18c9772c94739ab687ae58cfd7ea81\n",
      "  Stored in directory: /Users/chenhongyan/Library/Caches/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/chenhongyan/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (4.25.4)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (1.66.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: six>1.9 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/chenhongyan/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/chenhongyan/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 50479.14062665884, Loss: 797.0499\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义生产线环境 (示例)\n",
    "class ProductionLineEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ProductionLineEnv, self).__init__()\n",
    "        self.state_space = gym.spaces.Box(low=0, high=1, shape=(4,))\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(0, 1, 4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.calculate_reward(action)\n",
    "        done = False\n",
    "        self.state = np.random.uniform(0, 1, 4)\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        efficiency = self.state[action]  # 简单示例：使用当前状态的一个值作为效率\n",
    "        return efficiency * 100  # 奖励是效率的倍数\n",
    "\n",
    "# DQN 模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 主要流程\n",
    "def train_dqn(dqn, optimizer, memory, batch_size=128, gamma=0.99):\n",
    "    state, action, reward, next_state, done = memory.sample(batch_size)\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action = torch.LongTensor(action).to(device)\n",
    "    reward = torch.FloatTensor(reward).to(device)\n",
    "    done = torch.FloatTensor(done).to(device)\n",
    "\n",
    "    q_values = dqn(state)\n",
    "    next_q_values = dqn(next_state)\n",
    "    \n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# 主程序\n",
    "env = ProductionLineEnv()\n",
    "state_size = env.state_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dqn = DQN(state_size, action_size).to(device)\n",
    "optimizer = optim.Adam(dqn.parameters())\n",
    "memory = ReplayBuffer(1000)\n",
    "\n",
    "# 模拟训练过程\n",
    "num_episodes = 10000\n",
    "best_reward = -float('inf')\n",
    "best_episode = None\n",
    "losses = []  # 用于存储每一集的损失\n",
    "all_rewards = []  # 用于存储每一集的总奖励\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(1000):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = dqn(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if len(memory) > 128:\n",
    "            loss = train_dqn(dqn, optimizer, memory)\n",
    "            losses.append(loss)\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        best_episode = episode + 1\n",
    "\n",
    "# 输出最佳 episode 的结果\n",
    "print(f\"\\n最佳 Episode 是第 {best_episode} 集，总奖励为 {best_reward}\")\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# 绘制奖励曲线\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.plot(all_rewards)\n",
    "plt.title(\"every eposide total reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n",
    "\n",
    "# 如果需要，可以在这里保存模型参数\n",
    "torch.save(dqn.state_dict(), 'best_dqn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
